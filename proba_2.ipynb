{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerko/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lens(abc.ABC, torch.nn.Module):\n",
    "    \"\"\"Abstract base class for all Lens\"\"\"\n",
    "    \n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cpu')\n",
    "        self.output_logits = False\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, h, idx):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def from_model(model, init_parameter=None):\n",
    "        rv = Lens(model.config.hidden_size, model.config.hidden_size)\n",
    "        rv.dim_in = model.config.hidden_size\n",
    "        rv.dim_out = model.config.hidden_size\n",
    "        return rv\n",
    "\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        pass\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "    \n",
    "class Logit_lens(Lens):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        return h[:,idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_model(model, init_parameter=None):\n",
    "        rv = Logit_lens(model.config.hidden_size, model.config.hidden_size)\n",
    "        rv.dim_in = model.config.hidden_size\n",
    "        rv.dim_out = model.config.hidden_size\n",
    "        return rv\n",
    "    \n",
    "class Linear_lens(Lens):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__(dim_in, dim_out)\n",
    "        self.linear = torch.nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        return self.linear(h)[:,idx]\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        self.linear.to(device)\n",
    "        return self\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        self.linear.weight = torch.nn.Parameter(parameters['weight'])\n",
    "        self.linear.bias = torch.nn.Parameter(parameters['bias'])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_model(model, init_parameter=None):\n",
    "        rv = Linear_lens(model.config.hidden_size, model.config.hidden_size)\n",
    "        rv.dim_in = model.config.hidden_size\n",
    "        rv.dim_out = model.config.hidden_size\n",
    "        if init_parameter is not None:\n",
    "            rv.set_parameters(init_parameter)\n",
    "\n",
    "        return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lens_model(torch.nn.Module):\n",
    "    def __init__(self, lens, layers=None, model_name=\"gpt2\", model_path=None):\n",
    "        '''\n",
    "        Initializes the Lens_model class.\n",
    "        model_name: str\n",
    "            The name of the model to be used.\n",
    "        lens: list of Lens\n",
    "            The lens to be used.\n",
    "        layers: list of ints\n",
    "            The layers to be used.\n",
    "        '''\n",
    "        super(Lens_model, self).__init__()\n",
    "        \n",
    "        if model_path is not None:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "        \n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        assert len(lens) == len(layers)\n",
    "        self.lens = torch.nn.ParameterList(lens)\n",
    "        self.layers = layers\n",
    "        self.device = torch.device('cpu')\n",
    "        self.num_layers = self.model.config.num_hidden_layers\n",
    "        self.unembed = self.model.get_output_embeddings()\n",
    "        self.final_layer_norm = self.model.base_model.ln_f\n",
    "        self.unembed.requires_grad = False\n",
    "        self.final_layer_norm.requires_grad = False\n",
    "\n",
    "    def to(self, device):\n",
    "        '''\n",
    "        Moves the model to the device.\n",
    "        device: torch.device\n",
    "            The device to be used.\n",
    "        '''\n",
    "        self.device = device\n",
    "        for l in self.lens:\n",
    "            l.to(device)\n",
    "\n",
    "        self.model.to(device)\n",
    "        self.unembed.to(device)\n",
    "        self.final_layer_norm.to(device)\n",
    "    \n",
    "    def get_probs(self, input_ids, attention_mask, targets, target_index):\n",
    "        '''\n",
    "        Gets the probabilities of the targets.\n",
    "        input_ids: torch.Tensor\n",
    "            The input ids.\n",
    "        attention_mask: torch.Tensor\n",
    "            The attention mask.\n",
    "        targets: torch.Tensor\n",
    "            The targets.\n",
    "        target_index: torch.Tensor\n",
    "            The index of the token that we will predict\n",
    "        Output: torch.Tensor\n",
    "            The probabilities of the targets. The shape is (batch_size, vocab_size, num_layers)\n",
    "        '''\n",
    "        output = self.forward(input_ids, attention_mask, targets)\n",
    "        '''\n",
    "        for i in range(len(output)):\n",
    "            layer_ = self.layers[i]\n",
    "            batch_size = output[i].shape[0]\n",
    "            output[i] = torch.softmax(output[i][torch.arange(batch_size), target_index - 1], dim=-1)\n",
    "        '''\n",
    "        logits = output[torch.arange(output.shape[0]), target_index-1]\n",
    "        probs = torch.softmax(logits, dim=-2)\n",
    "        return probs\n",
    "    \n",
    "    def get_correct_class_probs(self, input_ids, attention_mask, targets, target_index):\n",
    "        '''\n",
    "        Gets the probabilities of the correct .\n",
    "        input_ids: torch.Tensor\n",
    "            The input ids.\n",
    "        attention_mask: torch.Tensor\n",
    "            The attention mask.\n",
    "        targets: torch.Tensor\n",
    "            The targets.\n",
    "        target_index: torch.Tensor\n",
    "            The index of the token that we will predict\n",
    "        Output: torch.Tensor\n",
    "            The probabilities of the correct class. The shape is (batch_size, num_layers)\n",
    "        '''\n",
    "        probs = self.get_probs(input_ids, attention_mask, targets, target_index)\n",
    "        return probs[torch.arange(probs.shape[0]), targets[torch.arange(targets.shape[0]), target_index]]\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, targets):\n",
    "        '''\n",
    "        Forward pass of the model.\n",
    "        input_ids: torch.Tensor\n",
    "            The input ids.\n",
    "        attention_mask: torch.Tensor\n",
    "            The attention mask.\n",
    "        targets: torch.Tensor\n",
    "            The targets.\n",
    "        index: int\n",
    "            The index of the target token.\n",
    "        Output: torch.Tensor\n",
    "            The output of the model. The shape is (batch_size, max_length, vocab_size, num_layers)\n",
    "        '''\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            model_outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=targets, output_hidden_states=True)\n",
    "\n",
    "        hs = torch.stack(model_outputs.hidden_states, dim = 1)\n",
    "        #breakpoint()\n",
    "        output = []\n",
    "        for ly, ln in zip(self.layers, self.lens):\n",
    "            lens_output = ln.forward(hs, ly)\n",
    "            if ln.output_logits:\n",
    "                output.append(lens_output)\n",
    "            \n",
    "            else:\n",
    "                if ly == -1 or ly == self.num_layers:\n",
    "                    #with torch.no_grad():\n",
    "                    logits = self.unembed.forward(lens_output)\n",
    "                    \n",
    "                    output.append(logits)\n",
    "                else:\n",
    "                    #with torch.no_grad():\n",
    "                    logits = self.unembed.forward(self.final_layer_norm.forward(lens_output))\n",
    "                    \n",
    "                    output.append(logits)\n",
    "        \n",
    "        return torch.stack(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lens_model(\n",
       "  (model): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       "  (lens): ParameterList(\n",
       "      (0): Object of type: Linear_lens\n",
       "      (1): Object of type: Linear_lens\n",
       "      (2): Object of type: Linear_lens\n",
       "      (3): Object of type: Linear_lens\n",
       "      (4): Object of type: Linear_lens\n",
       "      (5): Object of type: Linear_lens\n",
       "      (6): Object of type: Linear_lens\n",
       "      (7): Object of type: Linear_lens\n",
       "      (8): Object of type: Linear_lens\n",
       "      (9): Object of type: Linear_lens\n",
       "      (10): Object of type: Linear_lens\n",
       "      (11): Object of type: Linear_lens\n",
       "    (0): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (1): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (2): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (3): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (4): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (5): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (6): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (7): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (8): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (9): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (10): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (11): Linear_lens(\n",
       "      (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (unembed): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('./models/lens_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
